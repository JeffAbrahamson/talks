\input ../setup.tex

\title
{Statistics for Machine Learning and Big Data}
\subtitle{An Introduction\\[6mm] Part 5: Decision algorithms}

\author[Abrahamson] {Jeff Abrahamson}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 
%\beamerdefaultoverlayspecification{<+->}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\begin{frame}
  \frametitle{$k$-means}

  \cimg{k-means.jpg}

  \prevwork{\url{mathworks.com}}
  
  \cnote{

    Note that this is not related to kNN.

    Explain:
    \begin{itemize}
    \item Choose $k$
    \item Choose $k$ centroids
    \item Compute distances and assign each point to closest centroid
    \item Now re-compute centroids based on classes
    \item Now re-assign based on closest
    \item Converges because distances get smaller
    \end{itemize}

    Parameter sweep is a possibility for choosing $k$.

    Assumes roughly spherical clusters.

    Assignment:
    \begin{itemize}
    \item Pick $k$ random centroids from sample (\textit{Forgy} method).
    \item Assign to $k$ random clusters (\textit{random partitions} method).
    \end{itemize}

    Uses
    \begin{itemize}
    \item Vector quantization (picking hopefully prototypical samples)
    \end{itemize}

  }
\end{frame}

\begin{frame}
  \frametitle{Examples}

  \vfill
  \centerline{\huge{\bf Code time}}
\end{frame}

\begin{frame}
  \frametitle{Discussion}

  When should we use $k$-means vs logistic regression?
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\talksection{SVM}

\begin{frame}
  \frametitle{Support Vector Machines}

  \only<1> {
    \begin{itemize}
    \item Goal: optimal separating hyperplane
    \item aka: Large Margin Classifier
    \end{itemize}
  }

  \cnote{
    Discussion:

    Consider the example of dots at [[0, 1], [1, 0]].
  }
\end{frame}

%%%% Some later slides...

\begin{frame}
  \frametitle{Strategies}

  \only<1> {
    At the beginning, one tends to do this:
    \begin{itemize}
    \item Transform data for SVM solver
    \item Randomly try a few kernels and parameters
    \item Test
    \end{itemize}
  }
  \only<2> {
    A better strategy:
    \begin{itemize}
    \item Transform
    \item \blue{Scale data}
    \item \blue{Consider linear, Gaussian, or RBF kernels}
    \item \blue{Use cross validation to find the best $C$ and $\gamma$}
    \item Test
    \end{itemize}
  }
  \cnote{
    Grid search often works well for $C$ and $\gamma$.  Try
    exponentially increasing $C$ and decreasing $\gamma$.  E.g.,
    $2^{-5}, 2^{-3}, \dotsc, 2^{15}$.
  }
  \only<3> {
    Categories usually work better as binary fields than as enums.
  }
  \cnote {
    
    
    If there are lot of features, linear often works well.

    $n$ features, $m$ training examples.
    
    When $n$ (e.g., $10^4$) is much larger $m$ (e.g., 10 to $10^3$),
    then linear regression or SVM with linear kernel tends to work
    well.

    When $n$ is small ($n<10^3$) and $m$ medium ($m<10^4$), then
    Gaussian kernel often works well.

    When $m$ is large ($n < 10^3$ and $m>5\cdot 10^4$), add features
    and then use linear regression or SVM with a linear kernel.

  }
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\talksection{Break}

\begin{frame}
  \frametitle{Questions?}
  \vspace{3cm}
  \centerline{\large\url{purple.com/talk-feedback}}
\end{frame}

\end{document}

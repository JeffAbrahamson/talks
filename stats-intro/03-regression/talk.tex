\documentclass[t]{beamer}

\input ../setup.tex

\title
{Statistics for Machine Learning and Big Data}
\subtitle{An Introduction\\[6mm] Part 3: regression}

\author[Abrahamson] {Jeff Abrahamson}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 
%\beamerdefaultoverlayspecification{<+->}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\talksection{Regression}

\begin{frame}
  \frametitle{Linear models}

  \textbf{Problem:}  We have a set of points $\{(x_i, y_i)\}$.  Given a new $x$
  value, we'd like to predict $\hat y$.

  \textbf{Linear model:}  We'll assume there exists a linear relationship
  $y=\beta_0 + \beta_1 x$ that offers a good approximation to the data.

  \only<2>{
    We call $x$ the \textbf{explanatory} or \textbf{predictor}
    variable.

    We call $y$ the \textbf{response} variable.
  }

  \only<3>{Example:
    \cimgsm{regression-line-1.png}
  }

  \only<4>{Example:
    \cimgsm{regression-line-2.png}
  }

  \only<5>{Example:
    \cimgsm{regression-line-3.png}
  }

  \only<6>{Example:
    \cimg{regression-line-4.png}
  }

  \note{
    In the real world, there's nearly always noise.

    Sometimes there are other lesser effects as well.

    Talk about meaning of slope

    Dangers of extrapolation.  Example: global warming (a few data
    points in a few places at a few times)
  }

\end{frame}

\begin{frame}
  \frametitle{Residuals}

  What's left over.

  \only<1>{
    \vspace{1cm}
    \begin{displaymath}
      \text{data} = \text{fit} + \text{residual}      
    \end{displaymath}
  }
  \only<2>{
    \vspace{1cm}
    \begin{displaymath}
      y_i = \hat y_i + e_i
    \end{displaymath}
  }

  \note{Goal: small residuals!}
  
  \only<3>{\vfill\cimg{residuals-1.png}}
  \only<4>{\vfill\cimg{residuals-2.png}}
  
  \only<5>{
    Goal: small residuals.

    \vspace{1cm}
    \begin{displaymath}
      \sum \mid e_i\mid
    \end{displaymath}
  }
  \only<6>{
    Goal: small residuals.

    \vspace{1cm}
    \begin{displaymath}
      \sum e_i^2
    \end{displaymath}
  }

  \note{
    Residuals are what's left over after accounting for model fit.

    A normal distribution of residuals is a good sign.  And
    conversely.

    Not rules: rule of thumb.

    Time series often have important underlying structure.\\
    Correlation often doesn't model them well.
  }

\end{frame}

\begin{frame}
  \frametitle{Categorical regression}

  \vfill
  \cimg{categorical-regression.png}

  \note{
    Prices at ebay auctions.

    TODO: Is this the same as line connecting means?  If no, demonstrate.
  }

\end{frame}

\begin{frame}
  \frametitle{Outliers}

  \textit{High leverage} points fall far from the regression line.

  \textit{Influential points} make their leverage known.

  \note{
    Points that fall farther from the regression line have more
    effect.  We call them \textit{high leverage} points.

    If the effect is noticeable on the regression, we call it an
    \textit{influential point}.

    If a point, omitted, would fall much further from the regression
    line, it is certainly influential.

    If not enough data points, they might be all or mostly influential!
  }

\end{frame}

\begin{frame}
  \frametitle{Outliers}

  \only<1>{
    \vfill
    \cimgsm{outliers-1.png}
    \note{
      There is one outlier far from the other points, though it only
      appears to slightly influence the line.
    }
  }
  
  \only<2>{
    \vfill
    \cimgsm{outliers-2.png}
    \note{There is one outlier on the right, though it is quite close
      to the least squares line, which suggests it wasn’t very influential.
    }
  }
  
  \only<3>{
    \vfill
    \cimgsm{outliers-3.png}
    \note{There is one point far away from the cloud, and this outlier appears to pull the
      least squares line up on the right; examine how the line around the primary
      cloud doesn’t appear to fit very well.
    }
  }
  
  \only<4>{
    \vfill
    \cimgsm{outliers-4.png}
    \note{There is a primary cloud and then a small secondary cloud of four outliers. The
      secondary cloud appears to be influencing the line somewhat strongly, making
      the least square line fit poorly almost everywhere. There might be an interesting
      explanation for the dual clouds, which is something that could be investigated.
    }
  }
  
  \only<5>{
    \vfill
    \cimgsm{outliers-5.png}
    \note{There is no obvious trend in the main cloud of points and the outlier on the
      right appears to largely control the slope of the least squares line.
    }
  }
  
  \only<6>{
    \vfill
    \cimgsm{outliers-6.png}
    \note{There is one outlier far from the cloud, however, it falls quite close to the least
      squares line and does not appear to be very influential.
    }
  }

  \only<7>{
    \vfill\centerline{\huge Don't ignore outliers.}
  }

  \only<8>{
    \vfill
    \cimg{outliers-important.png}
  }

  \note{
    Data about shuttle O-ring damage.
  }

\end{frame}

\begin{frame}
  \frametitle{Correlation}

  \only<1>{
    Population correlation.
    \vspace{15mm}
    \begin{displaymath}
      \rho_{X,Y}
      = \corr(X,Y)
      = \frac{\cov(X,Y)}{\sigma_X \sigma_Y}
      = \frac{\E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X\sigma_Y}
    \end{displaymath}
  }
  \note{
    Correlation measures dependence between two random variables.

    Pearson's product-moment coefficient.\\
    Pearson's coefficient\\
    Pearson's correlation\\
    Correlation

    (But there are others.)\\
    Pearson's correlation is sensitive to linear relationships.

    If $\rho=1$, we say perfect (increasing) correlation.\\
    If $\rho=-1$, we say perfect (decreasing) correlation or perfect
    anticorrelation.

    Otherwise, indicates the degree of linear dependence.

    Independent $\Rightarrow$ Pearson's correlation coefficient is zero.\\
    Converse not true.  E.g., $y=x^2$ has zero correlation.

    If $X$ and $Y$ are \textit{jointly normal}, then zero correlation
    $\Rightarrow$ independent.

    (\textit{multivariate normal distribution}, \textit{multivariate
      Gaussian distribution})

    $r²$ describes amount of variation in the response explained by
    the least squares line.  It's called the \textit{coefficient of determination.}
  }
  \only<2>{
    Sample correlation.
    \vspace{15mm}
    \begin{displaymath}
      r_{x,y}
      = \frac{1}{n-1}
      \mathlarger{\mathlarger{\sum}}_{i=1}^{n}
      \left(\frac{x_i - \overline x}{s_x}\right)
      \left(\frac{y_i - \overline y}{s_y}\right)
    \end{displaymath}
  }

  \only<3>{
    Coefficient of determination
    \vspace{15mm}
    \begin{displaymath}
      r^2 = \frac{\var e_i}{\var y_i}
    \end{displaymath}
  }

  \note{
    This is (usually) true in one dimension under common conditions.
    In higher dimensions, we still call it $r^2$ but it's not longer
    the Pearson's correlation coefficient.

    $r^2$ measures goodness of fit.
  }

  \only<4>{\vfill\cimg{correlations-1.png}}
  \only<5>{\vfill\cimg{correlations-2.png}}
  \only<6>{Anscombe's Quartet
    
    \vfill\cimg{anscombe_quartet.png}
  }
  \only<7>{Anscombe's Quartet
    
    \vfill\cimgg{anscombe_quartet.png}

    \prevwork{\url{http://en.wikipedia.org/wiki/File:Anscombe\%27s_quartet_3.svg}}
  }

  \note{
    Created by Francis Anscombe.

    \begin{itemize}
    \item mean = 7.5
    \item variance = 4.12
    \item correlation = 0.816
    \item regression line = $y=3+0.5x$
    \end{itemize}

    Summary statistics don't replace visualising data.
  }

\end{frame}

\begin{frame}
  \frametitle{Correlation does not imply causation}

  \only<1>{
    \vfill
    \cimg{xkcd-552-correlation.png}

    \prevwork{\url{https://xkcd.com/552/}}
  }

  \only<2>{
    Tufte:

    \vspace{1cm}

    \begin{quote}
      "Empirically observed covariation is a necessary but not sufficient condition for causality."
    \end{quote}

    \vspace{1cm}
    \prevwork{\url{http://en.wikipedia.org/wiki/Correlation\_does\_not\_imply\_causation}}
  }
  \only<3>{
    Tufte:

    \vspace{1cm}

    \begin{quote}
      "Correlation is not causation but it sure is a hint."
    \end{quote}

    \vspace{1cm}
    \prevwork{\url{http://en.wikipedia.org/wiki/Correlation\_does\_not\_imply\_causation}}
  }

  \note{

    In the xkcd, the salient point is that the character who didn't
    take the statistics class can't validly make that conclusion.

    The character who took the statistics class, of course, probably can.

  }

\end{frame}

\begin{frame}
  \frametitle{Code lab: linear regression}

  \note{
    TODO: prepare data

    TODO: do this myself
  }

  \only<1> {
    Data set 1: \url{...} TODO

    Tasks:
    \begin{itemize}
    \item Visualise the data
    \item Compute mean, variance, standard deviation, correlation
    \item Compute linear regression and visualise
    \item Compute residuals and visualise
    \item Discuss what the data tells you
    \end{itemize}
  }

  \only<2> {
    Data set 2: \url{...} TODO

    Tasks:
    \begin{itemize}
    \item Visualise the data
    \item Compute mean, variance, standard deviation, correlation
    \item Compute linear regression and visualise
    \item Compute residuals and visualise
    \item Discuss what the data tells you
    \end{itemize}
  }

  \only<3> {
    Data set 3: \url{...} TODO

    Tasks:
    \begin{itemize}
    \item Visualise the data
    \item Compute mean, variance, standard deviation, correlation
    \item Compute linear regression and visualise
    \item Compute residuals and visualise
    \item Discuss what the data tells you
    \end{itemize}
  }

\end{frame}

\begin{frame}
  \frametitle{Linear regression and inference}

  TODO.  Talk about standard error and $p$ values.  Probably talk
  about $t$ tests.  Probably use sklearn and demo.
  
\end{frame}

\begin{frame}
  \frametitle{Multiple regression}

  One response $\hat y$ but many predictors $x_1, \dotsc, x_n$.

  \begin{displaymath}
    \hat y = \beta_0 + \beta_1 x_1 + \dotsb + \beta_n x_n
  \end{displaymath}

  \note{
    We fit all predictors simultaneously.\\
    Fitting individually is not valid.
  }

\end{frame}

\begin{frame}
  \frametitle{Logistic regression}

  \only<1>{
    Also: \textit{logit regression}, \textit{logit model}

    Dependent variable ($y$) is binary.
  }

  \note{
    Dependent variable ($y$) is binary $\Rightarrow$ logistic
    regression is a classifier.

    Logistic regression is an example of a \textit{generalized linear
      model} (GLM).

    Example: probability of dying given some non-boolean measure
    (e.g., size of tumour, time unconscious, etc.).

    If $y$ is binary, we can say \textit{binary} or \textit{binomial
      logistic regression}.\\
    If $y$ may take on 3 or more discrete values, we speak of
    \textit{multinomial logistic regression}.
  }

  \only<2>{
    \vfill
    \cimg{logistic-function.png}
  }

  \note{
    We compute residuals the same way: predicted value less observed value.
  }

\end{frame}

\begin{frame}
  \frametitle{Fitting polynomials}

  \begin{itemize}
  \item Under/over-fitting
  \item Test is not validation
  \end{itemize}

  \note{
    There exists a unique polynomial of minimum degree that fits a given (finite) set of points.
    \begin{displaymath}
      S = \{ (x_0, y_0), \dotsc, (x_k, y_k) \}
    \end{displaymath}

    Uniqueness proof is by counterexample: suppose $p(x)$ and $q(x)$
    are minimal degree and fit $S$.  Suppose further highest order
    coefficient is 1.  Subtract.  Zero at the $k+1$ points of $S$, so
    has degree at least $k+1$.  But has degree at most $k$, since
    highest order term is now zero.  So $p=q$.

    The \textbf{Newton form} of the interpolation polynomial is a linear
    combination of Newton basis polynomials:
    \begin{displaymath}
      N(x) = \sum_{j=0}^k a_j n_j(x)
    \end{displaymath}
    \begin{displaymath}
      n_j(x) = \prod_i=0^{j-1}(x-x_i)
    \end{displaymath}
    for $j>0$ and $n_0(x)\equiv 1$ and $a_j$ is defined in terms of
    divided differences..

    \bigskip

    The \textbf{Lagrange form} is a linear combination of Lagrange
    basis polynomials:
    \begin{displaymath}
      L(x) = \sum_{j=0}^k y_j l_j(x)
    \end{displaymath}
    \begin{displaymath}
      l_j(x) = \prod_{m\in [0,k], m\ne j} \frac{x-x_m}{x_j-x_0}
    \end{displaymath}

    Subject to Runge's phenomenon (oscillation near the interval
    edges).\\
    Higher degrees doesn't always improve accuracy.

    \bigskip

    We need a test set and a different validation set.  (Typically, we
    leave some data out for validation.)

    Barrier functions (e.g., sum of absolute values of coefficients)
    can help avoid over-fitting.  There are other techniques.  It
    depends on context.

  }
\end{frame}

\begin{frame}
  \frametitle{Questions?}
  \vspace{3cm}
  \centerline{\large\url{purple.com/talk-feedback}}
\end{frame}

\talksection{Break}

\end{document}

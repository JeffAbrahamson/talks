\documentclass[t]{beamer}

\mode<presentation>
{
  \usetheme{default}
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{footline}[frame number]
  \setbeamertemplate{items}[circle]
  \usecolortheme{seahorse}
}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{amsmath}
\usepackage{relsize}

\parskip=8 pt

\newcommand\topstrut{\rule{0pt}{2.6ex}}
\newcommand\bottomstrut{\rule[-1.2ex]{0pt}{0pt}}
\newcommand\doublestrut{\rule[-1.2ex]{0pt}{3.6ex}}

\newcommand\blue[1]{\textcolor{blue}{#1}}
\newcommand\red[1]{\textcolor{red}{#1}}
\newcommand\gray[1]{\textcolor{gray}{#1}}
\newcommand\smallgray[1]{\textcolor{gray}{\small\it #1}}
\newcommand\prevwork[1]{\smallgray{#1}}
\newcommand\cimg[1]{\centerline{\includegraphics[width=.9\textwidth]{#1}}}
\newcommand\cimgg[1]{\centerline{\includegraphics[width=.8\textwidth]{#1}}}
\newcommand\cimggg[1]{\centerline{\includegraphics[width=.7\textwidth]{#1}}}
\newcommand\cimgsm[1]{\centerline{\includegraphics[width=.4\textwidth]{#1}}}

\newcommand\talksection[1]{\section{#1}
\begin{frame}
  \vfill\Huge\bf\blue{\centerline{#1}}
\end{frame}
}

% The differential in an integral.
% After a function or a fraction, the \, may not be desired, see \DD.
% It is as much art, taste, and consistency as norms and science.
\newcommand\corr[0]{\mathbf{Corr}}
\newcommand\cov[0]{\mathbf{Cov}}
\newcommand\D[1]{\,\mathrm{d}{#1}}
\newcommand\DD[1]{\mathrm{d}{#1}}
\newcommand\E[0]{\mathbf{E}}
\newcommand\var[0]{\mathbf{Var}}
\newcommand\N[0]{\mathcal{N}}
\newcommand\R[0]{\mathbb{R}}

\title
{Statistics for Machine Learning and Big Data}
\subtitle{An Introduction\\[6mm] Part 3: regression}

\author[Abrahamson] {Jeff Abrahamson}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 
%\beamerdefaultoverlayspecification{<+->}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\talksection{Regression}

\begin{frame}
  \frametitle{Linear models}

  \textbf{Problem:}  We have a set of points $\{(x_i, y_i)\}$.  Given a new $x$
  value, we'd like to predict $\hat y$.

  \textbf{Linear model:}  We'll assume there exists a linear relationship
  $y=\beta_0 + \beta_1 x$ that offers a good approximation to the data.

  \only<2>{
    We call $x$ the \textbf{explanatory} or \textbf{predictor}
    variable.

    We call $y$ the \textbf{response} variable.
  }

  \only<3>{Example:
    \cimgsm{regression-line-1.png}
  }

  \only<4>{Example:
    \cimgsm{regression-line-2.png}
  }

  \only<5>{Example:
    \cimgsm{regression-line-3.png}
  }

  \only<6>{Example:
    \cimg{regression-line-4.png}
  }

  \note{
    In the real world, there's nearly always noise.

    Sometimes there are other lesser effects as well.
  }

\end{frame}

\begin{frame}
  \frametitle{Residuals}

  What's left over.

  \only<1>{
    \vspace{1cm}
    \begin{displaymath}
      \text{data} = \text{fit} + \text{residual}      
    \end{displaymath}
  }
  \only<2>{
    \vspace{1cm}
    \begin{displaymath}
      y_i = \hat y_i + e_i
    \end{displaymath}
  }

  \note{Goal: small residuals!}
  
  \only<3>{\vfill\cimg{residuals-1.png}}
  \only<4>{\vfill\cimg{residuals-2.png}}
  
  \only<5>{
    Goal: small residuals.

    \vspace{1cm}
    \begin{displaymath}
      \sum \mid e_i\mid
    \end{displaymath}
  }
  \only<6>{
    Goal: small residuals.

    \vspace{1cm}
    \begin{displaymath}
      \sum e_i^2
    \end{displaymath}
  }

  \note{
    Residuals are what's left over after accounting for model fit.

    A normal distribution of residuals is a good sign.  And
    conversely.

    Not rules: rule of thumb.
  }

\end{frame}

\begin{frame}
  \frametitle{Correlation}

  \only<1>{
    Population correlation.
    \vspace{15mm}
    \begin{displaymath}
      \rho_{X,Y}
      = \corr(X,Y)
      = \frac{\cov(X,Y)}{\sigma_X \sigma_Y}
      = \frac{\E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X\sigma_Y}
    \end{displaymath}
  }
  \note{
    Correlation measures dependence between two random variables.

    Pearson's product-moment coefficient.\\
    Pearson's coefficient\\
    Pearson's correlation\\
    Correlation

    (But there are others.)\\
    Pearson's correlation is sensitive to linear relationships.

    If $\rho=1$, we say perfect (increasing) correlation.\\
    If $\rho=-1$, we say perfect (decreasing) correlation or perfect
    anticorrelation.

    Otherwise, indicates the degree of linear dependence.

    Independent $\Rightarrow$ Pearson's correlation coefficient is zero.\\
    Converse not true.  E.g., $y=x^2$ has zero correlation.

    If $X$ and $Y$ are \textit{jointly normal}, then zero correlation
    $\Rightarrow$ independent.

    (\textit{multivariate normal distribution}, \textit{multivariate
      Gaussian distribution})
  }
  \only<2>{
    Sample correlation.
    \vspace{15mm}
    \begin{displaymath}
      r_{x,y}
      = \frac{1}{n-1}
      \mathlarger{\mathlarger{\sum}}_{i=1}^{n}
      \left(\frac{x_i - \overline x}{s_x}\right)
      \left(\frac{y_i - \overline y}{s_y}\right)
    \end{displaymath}
  }

  \only<3>{\vfill\cimg{correlations-1.png}}
  \only<4>{\vfill\cimg{correlations-2.png}}
  \only<5>{Anscombe's Quartet
    
    \vfill\cimg{anscombe_quartet.png}
  }
  \only<6>{Anscombe's Quartet
    
    \vfill\cimgg{anscombe_quartet.png}

    \prevwork{\url{http://en.wikipedia.org/wiki/File:Anscombe\%27s_quartet_3.svg}}
  }

  \note{
    Created by Francis Anscombe.

    \begin{itemize}
    \item mean = 7.5
    \item variance = 4.12
    \item correlation = 0.816
    \item regression line = $y=3+0.5x$
    \end{itemize}

    Summary statistics don't replace visualising data.
  }

\end{frame}

\begin{frame}
  \frametitle{Correlation does not imply causation}

  \only<1>{
    \vfill
    \cimg{xkcd-552-correlation.png}

    \prevwork{\url{https://xkcd.com/552/}}
  }

  \only<2>{
    Tufte:

    \vspace{1cm}

    \begin{quote}
      "Empirically observed covariation is a necessary but not sufficient condition for causality."
    \end{quote}

    \vspace{1cm}
    \prevwork{\url{http://en.wikipedia.org/wiki/Correlation\_does\_not\_imply\_causation}}
  }
  \only<3>{
    Tufte:

    \vspace{1cm}

    \begin{quote}
      "Correlation is not causation but it sure is a hint."
    \end{quote}

    \vspace{1cm}
    \prevwork{\url{http://en.wikipedia.org/wiki/Correlation\_does\_not\_imply\_causation}}
  }

  \note{

    In the xkcd, the salient point is that the character who didn't
    take the statistics class can't validly make that conclusion.

    The character who took the statistics class, of course, probably can.

  }

\end{frame}

\begin{frame}
  \frametitle{Granger causality}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{Convergent cross-mapping}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{Questions?}
  \vspace{3cm}
  \centerline{\large\url{purple.com/talk-feedback}}
\end{frame}

\talksection{Break}

\end{document}

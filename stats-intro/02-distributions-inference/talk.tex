\documentclass[t]{beamer}

\mode<presentation>
{
  \usetheme{default}
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{footline}[frame number]
  \setbeamertemplate{items}[circle]
  \usecolortheme{seahorse}
}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{amsmath}

\parskip=8 pt

\newcommand\topstrut{\rule{0pt}{2.6ex}}
\newcommand\bottomstrut{\rule[-1.2ex]{0pt}{0pt}}
\newcommand\doublestrut{\rule[-1.2ex]{0pt}{3.6ex}}

\newcommand\blue[1]{\textcolor{blue}{#1}}
\newcommand\red[1]{\textcolor{red}{#1}}
\newcommand\gray[1]{\textcolor{gray}{#1}}
\newcommand\smallgray[1]{\textcolor{gray}{\small\it #1}}
\newcommand\prevwork[1]{\smallgray{#1}}
\newcommand\cimg[1]{\centerline{\includegraphics[width=.9\textwidth]{#1}}}
\newcommand\cimgg[1]{\centerline{\includegraphics[width=.8\textwidth]{#1}}}
\newcommand\cimggg[1]{\centerline{\includegraphics[width=.7\textwidth]{#1}}}

\newcommand\talksection[1]{\section{#1}
\begin{frame}
  \vfill\Huge\bf\blue{\centerline{#1}}
\end{frame}
}

% The differential in an integral.
% After a function or a fraction, the \, may not be desired, see \DD.
% It is as much art, taste, and consistency as norms and science.
\newcommand\D[1]{\,\mathrm{d}{#1}}
\newcommand\DD[1]{\mathrm{d}{#1}}
\newcommand\E[0]{\mathbf{E}}
\newcommand\var[0]{\mathbf{Var}}
\newcommand\N[0]{\mathcal{N}}
\newcommand\R[0]{\mathbb{R}}

\title
{Statistics for Machine Learning and Big Data}
\subtitle{An Introduction\\[6mm] Part 2: distributions and inference}

\author[Abrahamson] {Jeff Abrahamson}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 
%\beamerdefaultoverlayspecification{<+->}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\talksection{Distributions}

\begin{frame}
  \frametitle{Uniform distribution}

  Takes value 1 with probability 1.
  \bigskip
  
  \begin{tabular}{ll}
    Model & identical events\\[1mm]
    Parameters & none\\[1mm]
    Mean & $\frac 12$\\[1mm]
    Variance & $1{12}$
  \end{tabular}

  TODO: need a picture of pdf, cdf
  
  \note{
    Does this mean I have probability 1 of picking any number on the unit interval?
  }
  
  TODO: show picture on $[0,n]$.

\end{frame}
\begin{frame}
  \frametitle{Bernoulli distribution}

  Takes value 1 with probability $p$ and 0 with probability $1-p$.
  \bigskip
  
  \begin{tabular}{ll}
    Model & turning coins\\[1mm]
    Parameters & $p\in [0,1]$\\[1mm]
    Mean & $p$\\[1mm]
    Variance & $p(1-p)$
  \end{tabular}

  TODO: need a picture of pdf, cdf
  
  \note{

  }
  
\end{frame}

\begin{frame}
  \frametitle{Geometric distribution}

  Two definitions:
  \begin{itemize}
  \item The probability distribution of the number $X$ of Bernoulli
    trials needed to get one success, supported on the set $\{1, 2, 3,
    \dotsc \}$
  \item The probability distribution of the number $Y = X - 1$ of
    failures before the first success, supported on the set $\{ 0, 1,
    2, 3, \dotsc \}$
  \end{itemize}

  \only<1>{\prevwork{Wikipedia, geometric distribution}}
  \only<2>{
    \begin{tabular}{ll}
      Model & \dots \\[1mm]
      Parameters & $p\in (0, 1]$ \\[1mm]
      Mean & $\frac 1p$ or $\frac{1-p}{p}$ \\[1mm]
      Variance & $\frac{1-p}{p^2}$
    \end{tabular}
    
  }

  TODO: need a picture of pdf, cdf
  
  \note{
    No canonical example of model.  Attempts to succeed at a task, for
    example.  Time to die from your daily commute.
    
    Does this model Russian roulette?  (No, not independent.)
  }
    
\end{frame}


\begin{frame}
  \frametitle{Poisson distribution}

  Probability of a given number of events occurring in a fixed
  interval of time and/or space if these events occur with a known
  average rate and independently of the time since the last event.
  \bigskip
  
  \begin{tabular}{ll}
    Model & radioactive decay, network packets\\[1mm]
    Parameters & $\lambda\in \R^+$\\[1mm]
    Mean & $\lambda$\\[1mm]
    Variance & $\lambda$
  \end{tabular}

  TODO: need a picture of pdf, cdf
  
  \note{
    Examples:  Deaths in Greater London, $\lambda$ is the mean rate
    of deaths per day.
  }
  
\end{frame}

\begin{frame}
  \frametitle{Binomial distribution}

  $\mathbf{B}(n,p) = $ Number of successes in a sequence of $n$ independent bernoulli
  trials (yes/no experiments), each of which yields success with
  probability $p$.
  \bigskip

  \begin{tabular}{ll}
    Model & sequences of coin tosses\\[1mm]
    Parameters & $n$, $p$\\[1mm]
    Mean & $np$\\[1mm]
    Variance & $np(1-p)$
  \end{tabular}

  TODO: Pictures of pdf, cdf.
  
  \note{
    TODO: Use matplotlib to show distributions for successive values
    of n.

    Often convenient to use the normal distribution as an approximation.
  }
  
\end{frame}

\begin{frame}
  \frametitle{Binomial distribution: Example}

  \only<1>{
    19\% of the British public smokes.  A study of 500 people reports 90
    smokers.  What is the probability of finding 90 or fewer smokers in
    a sample of 500 UK residents chosen at random?

    \prevwork{\url{http://en.wikipedia.org/wiki/Smoking_in_the_United_Kingdom}}
  }
  \only<2-3> {
    \red{20\%} of the British public smokes.  A study of 500 people
    reports 90~smokers.  What is the probability of finding 90 or
    fewer smokers in a sample of 500 UK residents chosen at random?
  }
  \only<3>{
    TODO: solution, first with binomial, then with normal.
    Show mathematically, then with scipy.

    The normal approximation is reasonable, since $(.2)(500) = 100
    >10$ and $(1-.2)(500) = 400 > 10$.

    Use $\mu=np=100$, $\sigma=\sqrt{80} \approx 8.9$, $\N(100, 8.9)$.
    Then $Z=\frac{59-100}{8.9} \approx -4.6$.  So probability is TODO.
  }

  \only<4>{
    \cimg{normal-binomial-discrepancy.png}
  }
  \note{
    Rule of thumb:  The binomial distribution is reasonably
    approximated by the normal distribution when both $np$ and
    $n(1-p)$ are greater than about 10.  Also a sufficient range
    (i.e., not just a couple values).  Can mitigate by decreasing
    lower cutoff by 0.5 and increasing upper cutoff by 0.5.
  }

\end{frame}

\begin{frame}
  \frametitle{Negative binomial distribution}

  How many Bernoulli trials with parameter $p$ until we have $r$
  successes?

  \begin{tabular}{ll}
    Model & \\[1mm]
    Parameters & $p$, $r$\\[1mm]
    Mean & $\frac{pr}{1-p}$\\[1mm]
    Variance & $\frac{pr}{(1-p)^2}$
  \end{tabular}

\end{frame}

\begin{frame}
  \frametitle{Comparing discrete distributions}

  \begin{description}
  \item[Binomial distribution.] \hfill \\
    Fixed number of trials, measures probability of success.
  \item[Negative binomial distribution.] \hfill \\
    Fixed number of successes, measures probable number of trials.
  \item[Poisson distribution.] \hfill \\
    Fixed number of trials, measures probable number of successes.
  \end{description}

\end{frame}

\begin{frame}
  \frametitle{Normal distribution}

  $\N(\mu,\sigma^2)$, about which we will say a great deal over the
  next hour and the rest of the day.
  \bigskip
  
  \begin{tabular}{ll}
    Model & cf. CLT\\[1mm]
    Parameters & $\mu$, $\sigma^2$\\[1mm]
    Mean & $\mu$\\[1mm]
    Variance & $\sigma^2$
  \end{tabular}

  TODO: Pictures of pdf, cdf.  Effects of varying parameters (do it live and note the commands here?).
  
  \note{
    Bell curve.  Carl Friedrich Gauss.
    
    Note that unimodal and roughly symmetric does not necessarily mean normal.

    Nonetheless, normal is often a good approximation.
  }
  
\end{frame}

\begin{frame}
  \frametitle{Z score}

  \begin{displaymath}
    Z = \frac{x-\mu}{\sigma}
  \end{displaymath}
  \note{
     We compute the $Z$ score for each observation.

     $Z$ scores are a coordinate transform to $\N(0,1)$.

     So we can use to compute percentiles.
  }
  
\end{frame}

\begin{frame}
  \frametitle{Example}

  The scores on an exam are approximately $\N(1500, 300)$.  What is
  the probability a random exam taker (one about whom we know nothing
  a priori) scores above 1630?

  \only<2>{
    \cimg{normal-SAT-1.png}
  }
  \only<3>{
    \begin{displaymath}
      Z = \frac{1630-1500}{300} = .43
    \end{displaymath}
  }
  \only<4>{
    \cimg{normal-SAT-2.png}
  }
  
  \note{
    TODO:  Demo this with scipy, too.

    We can also go backwards, from percentile to $Z$ score to values.
  }
  
\end{frame}

\begin{frame}
  \frametitle{65-95-99.7 Rule}

  \cimg{normal-RoT.png}
  \note{
    TODO:  Demo with scipy that $Z=1,2,3$ correspond to the rule.

    TODO:  Demo with scipy probabilities of falling outside $n\sigma$
    for $n=1,\dotsc,7$.
  }
  
\end{frame}

\begin{frame}
  \frametitle{Evaluating Normal Approximations}

  \only<1>{Easy technique 1: visually compare to normal plot.

    \cimg{normal-plot.png}
  }

  \note{
    Demo this in matplotlib.
  }

  \only<2> {Easy technique 2: normal probability plot.

    \cimggg{normal-quantile.png}

    Also known as a quantile-quantile plot.
  }

  \note{
    TODO:  Demo these in matplotlib, both prepared distributions (normal and
    not) and with norm() and different values of n.
  }

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\talksection{Inference}

\begin{frame}
  \frametitle{Inference}

  \only<1>{\vfill\centerline{\bf This is the important part.}}
  \only<2->{Goal: Understand the quality of parameter estimates.}

  \only<3>{
    Examples:
    \begin{itemize}
    \item How close is $\overline x$ to $\mu$?
    \end{itemize}
  }

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{Point estimates}

  Population mean ($\mu$) $\ne$ sample mean ($\overline x$).

  \note{
    We call $\overline x$ the \textit{point estimator} of the
    population mean $\mu$, a parameter of the distribution.

    Point estimate: if you have to guess, this is it.
  }

\end{frame}

\begin{frame}
  \frametitle{Inference Concepts}

  \only<1->{\textbf{Running mean.}  Sequence of partial sums (divided
    by number in sum).
  }

  \only<2>{
    \bigskip
    \cimgg{running-mean.png}
  }

  \only<3->{\textbf{Sampling variation.}  Change of $\overline x$ from
    one sample to the next.
  }

  \only<4->{\textbf{Sampling distribution.}  The distribution of
    possible point samples of a fixed size from a given population.
  }

  \vspace{5mm}
  \only<5>{\gray{``All problems in computer science can be solved by
    another level of indirection''  \textit{---David Wheeler}}
  }
  \only<6->{\gray{``All problems in computer science can be solved by
      another level of indirection, except of course for the problem
      of too many indirections.''  \textit{---David Wheeler}}
  }

  \only<7>{
    \vspace{5mm}

    \prevwork{\url{http://en.wikipedia.org/wiki/David_Wheeler_\%28British_computer_scientist\%29}}
  }

  \note{
    Smaller sampling variation means probably more accurate value.

    Think of sampling distribution as the set of subsets from which
    all samples come.

    Understanding sampling distributions is central to understanding inference.
  }
\end{frame}

\begin{frame}
  \frametitle{Sampling distribution}

  \only<1>{\vfill\cimg{sampling-distribution.png}}

  \note{
    \begin{itemize}
    \item Sampling mean is unimodal and approximately symmetric.
    \item It is centred at population mean.
    \item The standard deviation of the sample mean tells us how far a
      point sample's mean is likely to be from the population mean.
      In other words, how much error we are likely to have in the
      point estimate's mean.  \textbf{Standard error.}
    \end{itemize}

  }

  \only<2>{TODO: python code to generate uniform population, sample,
    and plot sampling distribution.}

  \only<3>{TODO: python code to generate highly skewed population, sample,
    and plot sampling distribution.}

  \note{
    In real life, we don't have access to the population parameters.
    We have to \textit{estimate} them from samples.  So we can't
    \textit{know} the standard error.

    Easy exercise:
    \begin{itemize}
    \item Would you rather have a small sample or a large sample when
      estimating a parameter?
    \item Would you expect a point sample based on a small sample to
      have smaller or larger standard error than a point sample based
      on a large sample?
    \end{itemize}

  }

  \only<4>{
    Given $n$ observations, the standard error of the sample means is
    \begin{displaymath}
      \text{S.E.} = \frac{\sigma}{\sqrt n}
    \end{displaymath}
  }
  \note{
    Cf. $\var(aX+bY) = a^2 \var(X) + b^2 \var(Y)$

    Usually use $\overline s$ instead of $\sigma$.
  }
\end{frame}

\begin{frame}
  \frametitle{Rules of Thumb}

  When is the preceding a good approximation?

  \begin{itemize}
  \item $n>30$
  \item The population is not too skewed.
  \end{itemize}

  When is the sample likely to be independent?

  \begin{itemize}
  \item Use simple random sampling.
  \item $n < $ 10\% of the population.
  \end{itemize}

  \note{

  }
\end{frame}

\begin{frame}
  \frametitle{Summary}

  \begin{itemize}
  \item Point estimates estimate population parameters.
  \item Point estimates have error and vary among samples.
  \item The standard error quantifies the variation among point estimates.
  \end{itemize}

  \note{

  }
\end{frame}

\begin{frame}
  \frametitle{Confidence intervals}

  Sample $n$ points, choose an interval around the sample mean.

  A 95\% confidence interval means if we sample repeatedly, about 95\%
  of the samples will contain the population mean.

  \only<2>{95\% confidence means $\pm 2$ S.E.}

  \only<3>{
    \cimg{confidence-intervals.png}
  }

  \note{
    Sampling is usually expensive.
  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{}

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{Questions?}
  \vspace{3cm}
  \centerline{\large\url{purple.com/talk-feedback}}
\end{frame}

\talksection{Lunch}

\end{document}

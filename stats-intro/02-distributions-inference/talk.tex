\documentclass[t]{beamer}

\input ../setup.tex

\title
{Statistics for Machine Learning and Big Data}
\subtitle{An Introduction\\[6mm] Part 2: distributions and inference}

\author[Abrahamson] {Jeff Abrahamson}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 
%\beamerdefaultoverlayspecification{<+->}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\talksection{Distributions}

\begin{frame}
  \frametitle{Uniform distribution}

  Takes value 1 with probability 1.
  \bigskip
  
  \purple{
    \begin{tabular}{ll}
      Model & identical events\\[1mm]
      Parameters & none\\[1mm]
      Mean & $\frac 12$\\[1mm]
      Variance & $\frac{1}{12}$
    \end{tabular}
  }
  
  TODO: need a picture of pdf, cdf
  
  \note{
    Does this mean I have probability 1 of picking any number on the unit interval?
  }
  
  TODO: show picture on $[0,n]$.

\end{frame}
\begin{frame}
  \frametitle{Bernoulli distribution}

  Takes value 1 with probability $p$ and 0 with probability $1-p$.
  \bigskip
  
  \begin{tabular}{ll}
    Model & turning coins\\[1mm]
    Parameters & $p\in [0,1]$\\[1mm]
    Mean & $p$\\[1mm]
    Variance & $p(1-p)$
  \end{tabular}

  TODO: need a picture of pdf, cdf
  
  \note{

  }
  
\end{frame}

\begin{frame}
  \frametitle{Geometric distribution}

  Two definitions:
  \begin{itemize}
  \item The probability distribution of the number $X$ of Bernoulli
    trials needed to get one success, supported on the set $\{1, 2, 3,
    \dotsc \}$
  \item The probability distribution of the number $Y = X - 1$ of
    failures before the first success, supported on the set $\{ 0, 1,
    2, 3, \dotsc \}$
  \end{itemize}

  \only<1>{\prevwork{Wikipedia, geometric distribution}}
  \only<2>{
    \begin{tabular}{ll}
      Model & \dots \\[1mm]
      Parameters & $p\in (0, 1]$ \\[1mm]
      Mean & $\frac 1p$ or $\frac{1-p}{p}$ \\[1mm]
      Variance & $\frac{1-p}{p^2}$
    \end{tabular}
    
  }

  TODO: need a picture of pdf, cdf
  
  \note{
    No canonical example of model.  Attempts to succeed at a task, for
    example.  Time to die from your daily commute.
    
    Does this model Russian roulette?  (No, not independent.)
  }
    
\end{frame}


\begin{frame}
  \frametitle{Poisson distribution}

  Probability of a given number of events occurring in a fixed
  interval of time and/or space if these events occur with a known
  average rate and independently of the time since the last event.
  \bigskip
  
  \begin{tabular}{ll}
    Model & radioactive decay, network packets\\[1mm]
    Parameters & $\lambda\in \R^+$\\[1mm]
    Mean & $\lambda$\\[1mm]
    Variance & $\lambda$
  \end{tabular}

  TODO: need a picture of pdf, cdf
  
  \note{
    Examples:  Deaths in Greater London, $\lambda$ is the mean rate
    of deaths per day.
  }
  
\end{frame}

\begin{frame}
  \frametitle{Binomial distribution}

  $\mathbf{B}(n,p) = $ Number of successes in a sequence of $n$ independent bernoulli
  trials (yes/no experiments), each of which yields success with
  probability $p$.
  \bigskip

  \begin{tabular}{ll}
    Model & sequences of coin tosses\\[1mm]
    Parameters & $n$, $p$\\[1mm]
    Mean & $np$\\[1mm]
    Variance & $np(1-p)$
  \end{tabular}

  TODO: Pictures of pdf, cdf.
  
  \note{
    TODO: Use matplotlib to show distributions for successive values
    of n.

    Often convenient to use the normal distribution as an approximation.
  }
  
\end{frame}

\begin{frame}
  \frametitle{Binomial distribution: Example}

  \only<1>{
    19\% of the British public smokes.  A study of 500 people reports 90
    smokers.  What is the probability of finding 90 or fewer smokers in
    a sample of 500 UK residents chosen at random?

    \vfill
    \prevwork{\url{http://en.wikipedia.org/wiki/Smoking_in_the_United_Kingdom}}
  }
  \only<2-3> {
    \red{20\%} of the British public smokes.  A study of 500 people
    reports 90~smokers.  What is the probability of finding 90 or
    fewer smokers in a sample of 500 UK residents chosen at random?
  }
  \only<3>{
    TODO: solution, first with binomial, then with normal.
    Show mathematically, then with scipy.

    The normal approximation is reasonable, since $(.2)(500) = 100
    >10$ and $(1-.2)(500) = 400 > 10$.

    Use $\mu=np=100$, $\sigma=\sqrt{80} \approx 8.9$, $\N(100, 8.9)$.
    Then $Z=\frac{59-100}{8.9} \approx -4.6$.  So probability is TODO.
  }

  \only<4>{
    \cimg{normal-binomial-discrepancy.png}
  }
  \note{
    Rule of thumb:  The binomial distribution is reasonably
    approximated by the normal distribution when both $np$ and
    $n(1-p)$ are greater than about 10.  Also a sufficient range
    (i.e., not just a couple values).  Can mitigate by decreasing
    lower cutoff by 0.5 and increasing upper cutoff by 0.5.
  }

\end{frame}

\begin{frame}
  \frametitle{Negative binomial distribution}

  How many Bernoulli trials with parameter $p$ until we have $r$
  successes?

  \begin{tabular}{ll}
    Model & \\[1mm]
    Parameters & $p$, $r$\\[1mm]
    Mean & $\frac{pr}{1-p}$\\[1mm]
    Variance & $\frac{pr}{(1-p)^2}$
  \end{tabular}

\end{frame}

\begin{frame}
  \frametitle{Comparing discrete distributions}

  \begin{description}
  \item[Binomial distribution.] \hfill \\
    Fixed number of trials, measures probability of success.
  \item[Negative binomial distribution.] \hfill \\
    Fixed number of successes, measures probable number of trials.
  \item[Poisson distribution.] \hfill \\
    Fixed number of trials, measures probable number of successes.
  \end{description}

\end{frame}

\begin{frame}
  \frametitle{Normal distribution}

  $\N(\mu,\sigma^2)$, about which we will say a great deal over the
  next hour and the rest of the day.
  \bigskip
  
  \begin{tabular}{ll}
    Model & cf. CLT\\[1mm]
    Parameters & $\mu$, $\sigma^2$\\[1mm]
    Mean & $\mu$\\[1mm]
    Variance & $\sigma^2$
  \end{tabular}

  TODO: Pictures of pdf, cdf.  Effects of varying parameters (do it live and note the commands here?).

\note{
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.mlab as mlab
import math

mean = 0
variance = 1
sigma = math.sqrt(variance)
x = np.linspace(-3,3,100)
plt.plot(x,mlab.normpdf(x,mean,sigma))

plt.show()



import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

\# Plot between -10 and 10 with .001 steps.
range = np.arange(-10, 10, 0.001)
\# Mean = 0, SD = 2.
plt.plot(range, norm.pdf(range,0,2))

}
  
  \note{
    Bell curve.  Carl Friedrich Gauss.
    
    Note that unimodal and roughly symmetric does not necessarily mean normal.

    Nonetheless, normal is often a good approximation.
  }
  
\end{frame}

\begin{frame}
  \frametitle{Z score}

  \vspace{1cm}
  
  \begin{displaymath}
    Z = \frac{x-\mu}{\sigma}
  \end{displaymath}
  \note{
     We compute the $Z$ score for each observation.

     $Z$ scores are a coordinate transform to $\N(0,1)$.

     So we can use to compute percentiles.
  }
  
\end{frame}

\begin{frame}
  \frametitle{Example}

  The scores on an exam are approximately $\N(1500, 300)$.  What is
  the probability a random exam taker (one about whom we know nothing
  a priori) scores above 1630?

  \only<2>{
    \cimg{normal-SAT-1.png}
  }
  \only<3>{
    \begin{displaymath}
      Z = \frac{1630-1500}{300} = .43
    \end{displaymath}
  }
  \only<4>{
    \cimg{normal-SAT-2.png}
  }
  
  \note{
    TODO:  Demo this with scipy, too.

    We can also go backwards, from percentile to $Z$ score to values.
  }
  
\end{frame}

\begin{frame}
  \frametitle{65-95-99.7 Rule}

  \vspace{1cm}
  
  \cimg{normal-RoT.png}
  \note{
    TODO:  Demo with scipy that $Z=1,2,3$ correspond to the rule.

    TODO:  Demo with scipy probabilities of falling outside $n\sigma$
    for $n=1,\dotsc,7$.
  }
  
\end{frame}

\begin{frame}
  \frametitle{Evaluating Normal Approximations}

  \only<1>{Easy technique 1: visually compare to normal plot.

    \cimg{normal-plot.png}
  }

  \note{
    Demo this in matplotlib.
  }

  \only<2> {Easy technique 2: normal probability plot.

    \cimggg{normal-quantile.png}

    Also known as a quantile-quantile plot.
  }

  \note{
    TODO:  Demo these in matplotlib, both prepared distributions (normal and
    not) and with norm() and different values of n.
  }

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\talksection{Inference}

\begin{frame}
  \frametitle{Inference}

  \only<1>{\vfill\centerline{\bf This is the important part.}}
  \only<2->{Goal: Understand the quality of parameter estimates.}

  \only<3>{
    Examples:
    \begin{itemize}
    \item How close is $\overline x$ to $\mu$?
    \end{itemize}
  }

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{Point estimates}

  Population mean ($\mu$) $\ne$ sample mean ($\overline x$).

  \note{
    We call $\overline x$ the \textit{point estimator} of the
    population mean $\mu$, a parameter of the distribution.

    Point estimate: if you have to guess, this is it.
  }

\end{frame}

\begin{frame}
  \frametitle{Inference Concepts}

  \only<1->{\textbf{Running mean.}  Sequence of partial sums (divided
    by number in sum).
  }

  \only<2>{
    \bigskip
    \cimgg{running-mean.png}
  }

  \only<3->{\textbf{Sampling variation.}  Change of $\overline x$ from
    one sample to the next.
  }

  \only<4->{\textbf{Sampling distribution.}  The distribution of
    possible point samples of a fixed size from a given population.
  }

  \vspace{5mm}
  \only<5>{\gray{``All problems in computer science can be solved by
    another level of indirection''  \textit{---David Wheeler}}
  }
  \only<6->{\gray{``All problems in computer science can be solved by
      another level of indirection, except of course for the problem
      of too many indirections.''  \textit{---David Wheeler}}
  }

  \only<7>{
    \vspace{5mm}

    \prevwork{\url{http://en.wikipedia.org/wiki/David_Wheeler_\%28British_computer_scientist\%29}}
  }

  \note{
    Smaller sampling variation means probably more accurate value.

    Think of sampling distribution as the set of subsets from which
    all samples come.

    Understanding sampling distributions is central to understanding inference.
  }
\end{frame}

\begin{frame}
  \frametitle{Sampling distribution}

  \only<1>{\vfill\cimg{sampling-distribution.png}}

  \note{
    \begin{itemize}
    \item Sampling mean is unimodal and approximately symmetric.
    \item It is centred at population mean.
    \item The standard deviation of the sample mean tells us how far a
      point sample's mean is likely to be from the population mean.
      In other words, how much error we are likely to have in the
      point estimate's mean.  \textbf{Standard error.}
    \end{itemize}

  }

  \only<2>{TODO: python code to generate uniform population, sample,
    and plot sampling distribution.}

  \only<3>{TODO: python code to generate highly skewed population, sample,
    and plot sampling distribution.}

  \note{
    In real life, we don't have access to the population parameters.
    We have to \textit{estimate} them from samples.  So we can't
    \textit{know} the standard error.

    Easy exercise:
    \begin{itemize}
    \item Would you rather have a small sample or a large sample when
      estimating a parameter?
    \item Would you expect a point sample based on a small sample to
      have smaller or larger standard error than a point sample based
      on a large sample?
    \end{itemize}

  }

  \only<4>{
    Given $n$ observations, the standard error of the sample means is
    \begin{displaymath}
      \text{S.E.} = \frac{\sigma}{\sqrt n}
    \end{displaymath}
  }
  \note{
    Cf. $\var(aX+bY) = a^2 \var(X) + b^2 \var(Y)$

    Usually use $\overline s$ instead of $\sigma$.
  }
\end{frame}

\begin{frame}
  \frametitle{Rules of Thumb}

  When is the preceding a good approximation?

  \begin{itemize}
  \item $n>30$
  \item The population is not too skewed.
  \end{itemize}

  When is the sample likely to be independent?

  \begin{itemize}
  \item Use simple random sampling.
  \item $n < $ 10\% of the population.
  \end{itemize}

  \note{

  }
\end{frame}

\begin{frame}
  \frametitle{Summary}

  \begin{itemize}
  \item Point estimates estimate population parameters.
  \item Point estimates have error and vary among samples.
  \item The standard error quantifies the variation among point estimates.
  \end{itemize}

  \note{

  }
\end{frame}

\begin{frame}
  \frametitle{Confidence intervals}

  Sample $n$ points, choose an interval around the sample mean.

  A 95\% confidence interval means if we sample repeatedly, about 95\%
  of the samples will contain the population mean.

  \only<2>{95\% confidence means $\pm 2$ S.E.}

  \only<3>{95\% confidence means $\pm 1.96$ S.E.}

  \only<4>{
    \cimg{confidence-intervals.png}
  }

  \only<5>{
    \cimg{confidence-intervals-2.png}
  }

  \note{
    Sampling is usually expensive.

    Reminder:  Independent random samples!

    We call $z^*$ S.E. the \textit{margin of error}.

    Correct language: ``We are 95\% confident that the population
    parameter is between\dots''
    
    Incorrect language: describe the confidence interval as capturing
    the population parameter with a certain probability.

    This is one of the most common errors: while it might be useful to
    think of it as a probability, the confidence level only quantifies
    how plausible it is that the parameter is in the interval.

    Another especially important consideration of confidence intervals
    is that they only try to capture the population parameter. Our
    intervals say \textit{nothing} about the confidence of
    \begin{itemize}
    \item capturing individual observations
    \item a proportion of the observations
    \item about capturing point estimates
    \end{itemize}
    Confidence intervals only attempt to capture population
    parameters.
  }

\end{frame}

\begin{frame}
  \frametitle{$p$-value}

  \only<1>{
    Do people die later now than 10 years ago?

    Compute the mean death age from 10 years ago.\\
    Compute a sample mean now and confidence interval.

    \begin{quote}
      $H_0$: no\\
      $H_A$: yes
    \end{quote}

    If the mean is within the confidence interval, we reject $H_A$.\\
    If the mean is outside the confidence interval, we accept $H_A$.
  }

  \note{$H_0$ should usually be  an equality.

    But maybe the mean falls near the edge, or way outside.  How
    to hint?
  }

  \only<2>{
    The $p$-value is a way of quantifying the strength of the evidence
    against the null hypothesis and in favor of the alternative.
  }

  \only<3>{
    The $p$-value is the probability of observing data at least as
    favorable to the alternative hypothesis as our current data set,
    if the null hypothesis is true.
  }

  \note{
    The $p$-value is a conditional probability.

    It answers ``what is the probability of the observed value if
    $H_0$ is true, assuming a normal sample distribution?''

    So it is the area under a bit of the tail of a Gaussian.
  }

\end{frame}

\begin{frame}
  \frametitle{Example: $p$-value}

  \only<1-2>{
    
    Googlers gain on average 7~kgs within one year of working at the
    company.  \only<2>{\red{\textit{(This may not be true, but it's a
          common joke.)}}}

    Facebook thinks their food is better, they say their employees
    gain more weight in the first year.  \only<2>{\red{\textit{This
          would be a perverse argument for joining facebook.}}}

    \begin{quote}
      $H_0$: $\mu=7$\\
      $H_A$: $\mu>7$      
    \end{quote}

    Sample $n=110$ facebookers, $\overline x = 7.42$ and $s = 1.75$.

    \only<2>{$\text{S.E.} = \frac{s}{\sqrt n} =
      \frac{1.75}{\sqrt 110} \approx 0.17$}
  }

  \note{
    $\mu>7$ is a one-sided hypothesis.  Use two-sided usually.  Decide
    before data collection.

    Check that normal approximation is ok:
    \begin{itemize}
    \item simple random sample, $<$ 10\%, so independent
    \item sample size $>30$
    \item moderate skew, outliers not too extreme
    \end{itemize}

  }

  \only<3>{
    \vfill
    \cimg{facebook-1.png}
  }

  \only<4>{
    \vfill
    \cimg{facebook-2.png}
  }

  \note{Shaded tail represents chance of observing $\overline x$
    conditional on $H_0$ being true.

    That is, it's the $p$-value.
  }

  \only<5>{
    \vfill
    \begin{displaymath}
      Z = \frac{\overline x - \mu}{\text{S.E.}} = \frac{7.42 -
        7}{0.17} \approx 2.47
    \end{displaymath}

    \vspace{1cm}
    \centerline{$\Pr(\overline x < 7.42) = .993$, so $p=1-.993=.007$.}
  }

  \note{
    So reject $H_0$.

    Note that $p<.05$ is very arbitrary and now often unacceptable in
    science.

    What is the cost of a Type I error (incorrectly rejecting a true $H_0$)?

    What is the cost of a Type II error (failure to reject a false $H_0$)?

    Useful to restate hypothesis in plain language so people understand.
  }

  \only<6>{
    \vfill
    \cimg{facebook-3.png}
  }
  \only<7>{
    \vfill
    \cimg{facebook-4.png}
  }

\end{frame}

\begin{frame}
  \frametitle{Central Limit Theorem (CLT)}

  \only<1>{
    The distribution of $\overline x$ is approximately normal. The
    approximation can be poor if the sample size is small, but it
    improves with larger sample sizes.

    \vfill
    \prevwork{Open Statistics}
  }
  
  \only<2>{
    Given certain conditions, the arithmetic mean of a sufficiently
    large number of iterates of independent random variables, each
    with a well-defined expected value and well-defined variance, will
    be approximately normally distributed, regardless of the
    underlying distribution.

    \vfill
    \prevwork{\url{http://en.wikipedia.org/wiki/Central_limit_theorem}}
  }

  \only<3>{
    Suppose $\{X_1, X_2, \dotsc\}$ is a sequence of i.i.d. random
    variables with $\E[X_i] = \mu$ and $\var[X_i] = \sigma^2 <
    \infty$. Then as $n$ approaches infinity, the random variables
    $\sqrt n(S_n - \mu)$ converge in distribution to a normal $\N(0, \sigma^2)$:

    \begin{displaymath}
      \sqrt{n}\bigg(\bigg(\frac{1}{n}\sum_{i=1}^n X_i\bigg) - \mu\bigg)\ \xrightarrow{d}\ N(0,\;\sigma^2). 
    \end{displaymath}

    \prevwork{\url{http://en.wikipedia.org/wiki/Central_limit_theorem}}
  }

  \note{

  }

\end{frame}

\begin{frame}
  \frametitle{Vocabulary}

  A point estimate is \textbf{unbiased} if the sampling distribution
  of the estimate is centred at the parameter it estimates.

  \note{
    An unbiased point estimate does not significantly over- or
    under-estimate the parameter.
  }

\end{frame}

\begin{frame}
  \frametitle{Code lab: A/B testing}

  TODO: Get data sets.  Show a/b test.  Question: what is $H_0$,
  $H_A$?  Should we adopt method B based on the evidence?
\end{frame}

\begin{frame}
  \frametitle{Questions?}
  \vspace{3cm}
  \centerline{\large\url{purple.com/talk-feedback}}
\end{frame}

\talksection{Lunch}

\end{document}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "import scipy.stats as ss\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to build a spam filter using scikit-learn.  This won't\n",
    "be as sophisticated as modern production filters, but it will be\n",
    "roughly at the level of Paul Graham's 2002 proposal [A Plan for Spam](http://www.paulgraham.com/spam.html), which gave rise to the\n",
    "still-useful bogofilter.\n",
    "\n",
    "One of our goals will be to show off the nice uniform interface\n",
    "that scikit-learn exports.  At the end we'll see that we can easily\n",
    "experiment with different classifiers with a minimum of bother.\n",
    "\n",
    "The data set we used in class, the Enron-Spam corpus, is publicly accessible.  SpamAssassin also offers a public corpus if you want\n",
    "to explore with even more data.\n",
    "\n",
    "#Read all the files\n",
    "\n",
    "Our first step is to read the data into python.  Here we write a\n",
    "generator function to recursively read each file in a directory,\n",
    "yielding after each.  This is a common python idiom, since it lets\n",
    "us use the read_files function easily.  Note that we yield a pair\n",
    "of items: the file path and the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEWLINE = b'\\n'\n",
    "SKIP_FILES = set(['cmds'])\n",
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, 'rb')\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    yield file_path, NEWLINE.join(lines).decode('cp1252', 'ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Make a dataframe of the spam.\n",
    "\n",
    "It is often convenient to manipulate data using pandas DataFrames.  A\n",
    "DataFrame is very much like a SQL table, and it permits most of the same\n",
    "sorts of operations, including projection, selection, and joins.\n",
    "\n",
    "Here we'll iterate over the results of `read_files()` (note how\n",
    "convenient it is to use by being a generator) and make a DataFrame\n",
    "with columns `text` and `class`.  Index columns in pandas are special\n",
    "(being indexes).  Here we index on the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data_frame(path, classification):\n",
    "    data_frame = DataFrame({'text': [], 'class': []})\n",
    "    for file_name, text in read_files(path):\n",
    "        data_frame = data_frame.append(\n",
    "                DataFrame({'text': [text], 'class': [classification]}, index=[file_name]))\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we just need to call our `build_data_frame()` function.  Notice\n",
    "that we've conveniently organized our spam and ham emails by the paths\n",
    "where they're stored.  This lets us make a simple table so that we know\n",
    "the class by the beginning of the filename.\n",
    "\n",
    "Note that we're assuming you've put the spam directories in a directory\n",
    "called \"data\" in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HAM = 0\n",
    "SPAM = 1\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spam',         SPAM),\n",
    "    ('data/easy_ham',     HAM),\n",
    "    ('data/hard_ham',     HAM),\n",
    "    ('data/beck-s',       HAM),\n",
    "    ('data/farmer-d',     HAM),\n",
    "    ('data/kaminski-v',   HAM),\n",
    "    ('data/kitchen-l',    HAM),\n",
    "    ('data/lokay-m',      HAM),\n",
    "    ('data/williams-w3',  HAM),\n",
    "    ('data/BG',           SPAM),\n",
    "    ('data/GP',           SPAM),\n",
    "    ('data/SH',           SPAM)\n",
    "    ]\n",
    "\n",
    "data = DataFrame({'text': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data = data.append(build_data_frame(path, classification))\n",
    "  \n",
    "data = data.reindex(np.random.permutation(data.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#The Matrix\n",
    "\n",
    "Scikit-learn isn't happy unless it is operating on matrices.  So our\n",
    "first job whenever we use scikit-learn is to convert our data to matrix\n",
    "form.  In this case, we want to count words in documents, since we are\n",
    "going to use Bayes Theorem to ask \"What is the probability that this new\n",
    "document is spam given these words in the document?\"  Our prior will be the words in the spam corpus we read with `build_data_frame()`.\n",
    "\n",
    "A `CountVectorizer` object builds a _sparse_ count of the words in the\n",
    "documents we feed it.  The `fit_and_transform()` function is slightly\n",
    "more efficient than calling `fit()` and `transform()` separately.  The `fit()` function learns the vocabulary from the raw documents.  If we had\n",
    "a prior vocabulary, we could potentially use that instead and so not call\n",
    "`fit()`.  `Transform()` extracts token counts using the vocabulary we\n",
    "learned from `fit()`.  What gets returned is a numpy sparse matrix.\n",
    "This is critical, because we probably don't have enough RAM to hold\n",
    "the dense matrix (that is, with all the zeros represented)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "# Fit learns the vocabulary.\n",
    "# Transform does the count.\n",
    "counts = count_vectorizer.fit_transform(np.asarray(data['text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "In machine learning, a classifier is an algorithm that takes data\n",
    "(representations of objects in the real world) and separates them\n",
    "into _classes_.  A class is anything, in our case spam or ham.\n",
    "It is a _supervised_ learning algorithm, which means that we start\n",
    "by _training_ it on data where we know the classes, and then we\n",
    "ask it to classify data where we don't know the classes.  The training\n",
    "phase is also called _fitting_.\n",
    "\n",
    "Naive Bayes is a classifier that uses Bayes Theorem to ask \"what is the\n",
    "probability that this document belongs to class $C$ (that is, our hypothesis $H$) given the evidence (document) $E$:\n",
    "\n",
    "$$\n",
    "\\Pr(H\\mid E) = \\frac{\\Pr(E\\mid H) \\Pr(H)}{\\Pr(E)}\n",
    "$$\n",
    "\n",
    "Here $H$ is our hypothesis (that the document is spam) and $E$ is the evidence (the document).  The individual terms have names that you\n",
    "may sometimes see:\n",
    "* $\\Pr(H)$ is the _prior_ probability of $H$\n",
    "* $\\Pr(E\\mid H)$ the _likelihood_ of observing $E$ given $H$\n",
    "* $\\Pr(E)$ the _marginal likelihood_ or _model evidence_\n",
    "* $\\Pr(H\\mid E)$ the _posterior_ probability of $H$\n",
    "\n",
    "The ratio of $\\Pr(E\\mid H) / \\Pr(E)$ we call the _impact_ of $E$ on $H$.\n",
    "\n",
    "In the end, if the evidence $E$ doesn't fit $H$, we reject $H$.\n",
    "If the evidence $E$ is extremely unlikely, we also will reject $H$ (and Bayes Theorem will tell us this!).  An example is an exceedingly rare disease on which we receive a positive test.  The error in the test (false positive rate) may carry more weight than the test's conclusion.  A more plebian example is the common (yet slightly paranoid) concern that a loved one has befallen some grave ill because he is late arriving.  Often we are Bayesians without realizing it, but our emotions are not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "targets = np.asarray(data['class'])\n",
    "classifier.fit(counts, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's test our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = ['Free Viagra call today!', \"I'm going to attend the Linux users group tomorrow.\"]\n",
    "example_counts = count_vectorizer.transform(examples)\n",
    "predictions = classifier.predict(example_counts)\n",
    "predictions # [1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pipelines offer a shortcut in scikit-learn.  We can specify the\n",
    "set of things we want to do, then do it.  Notice how this is clearer\n",
    "now that we know what we're doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "  ('vectorizer',  CountVectorizer()),\n",
    "  ('classifier',  MultinomialNB()) ])\n",
    "\n",
    "pipeline.fit(np.asarray(data['text']), np.asarray(data['class']))\n",
    "pipeline.predict(examples) # [1, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Cross Validation\n",
    "\n",
    "Any time we train a model, we must test that it is performing\n",
    "correctly.  How do we do this, though, unless we have some data set\n",
    "aside for which we know the correct results?  The solution to that\n",
    "problem is to set some of our training data aside and to use it for\n",
    "testing rather than for training.  To make sure we set aside the right\n",
    "bit of data, we simply try it with different sets.\n",
    "\n",
    "In one technique, we train on all but one bit of data, then test on the remaining (set aside) bit.  This can take a long time, though, since we have to retrain $N$ times.  Instead, we can set aside $1/k$ of our data and test against it.  This is called $k$-fold cross validation.  Even $k$-fold cross validation is likely to be the longest step in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98862048455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeff/src/jma/talks/stats-intro/venv/lib/python3.4/site-packages/sklearn/cross_validation.py:69: DeprecationWarning: The indices parameter is deprecated and will be removed (assumed True) in 0.17\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "k_fold = KFold(n=len(data), n_folds=6, indices=False)\n",
    "scores = []\n",
    "for train_indices, test_indices in k_fold:\n",
    "  train_text = np.asarray(data[train_indices]['text'])\n",
    "  train_y    = np.asarray(data[train_indices]['class'])\n",
    "\n",
    "  test_text  = np.asarray(data[test_indices]['text'])\n",
    "  test_y     = np.asarray(data[test_indices]['class'])\n",
    "\n",
    "  pipeline.fit(train_text, train_y)\n",
    "  score = pipeline.score(test_text, test_y)\n",
    "  scores.append(score)\n",
    "\n",
    "score = sum(scores) / len(scores)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Trying other models\n",
    "\n",
    "The pipeline construct lets us easily try other models.  Try these models and then cross-validate again to see the results.  Notice that the cross validation cell fits the data for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "  ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "  ('classifier',         MultinomialNB()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "  ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "  ('tfidf_transformer',  TfidfTransformer()),\n",
    "  ('classifier',         MultinomialNB()) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

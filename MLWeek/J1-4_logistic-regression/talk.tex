\input ../talk-header.tex
\title
{ML Week}
\subtitle{Logistic Regression}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 
%\beamerdefaultoverlayspecification{<+->}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\begin{frame}
  \frametitle{Linear regression}
  \begin{bphrase}
    \begin{itemize}
    \item Continuous output
    \item Normal residues
    \item Predict $\hat{y}$ for $x$ given $\{(x_i, y_i)\}$
    \end{itemize}
  \end{bphrase}
\end{frame}

\begin{frame}
  \frametitle{Logistic regression}
  \begin{bphrase}
    \begin{itemize}
    \item Binary output
    \item Classification
    \end{itemize}
  \end{bphrase}
\end{frame}

\begin{frame}
  \frametitle{Logistic regression}
  \begin{itemize}
  \item Have: continuous and discrete inputs
  \item Want: class (0 or 1)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Probabilistic inspiration}
  \only<1>{
    \sphrase{{$h_\theta(x) = .75$} $\iff$ {event has 75\% of being true}}
  }
  \only<2>{
    \sphrase{$h_\theta(x) = \Pr(y=1\mid x; \theta)$ = 0.75}
  }
  \only<3>{
    \sphrase{So this must be true:}
    
    \sphrase{$\Pr(y=0\mid x; \theta) + \Pr(y=1\mid x; \theta)$ = 1}
  }
  \only<4>{
    \sphrase{Set $y=1 \iff h_\theta(x) = \Pr(y=1\mid x; \theta) > \frac{1}{2}$}
  }
  \only<5>{
    Math review:
    \begin{itemize}
    \item $z=(\theta^T x)$
    \item $\theta^T x \ge 0 \iff h_\theta \ge 0.5$
    \item $\theta^T x \ge 0 \iff $ predict $y=1$
    \end{itemize}

    }

\end{frame}

\begin{frame}[t]
  \frametitle{Logistic (sigmoid, logit) function}
  \vspace{2cm}
  \begin{mphrase}
    g(z) = \frac{1}{1+e^{-z}}
  \end{mphrase}

  \only<2>{
    \vspace{8mm}
    Exercise: plot this}
\end{frame}

\begin{frame}
  \frametitle{Non-linear decision boundaries}
  \only<1> {
    \cimggg{non-linear-boundary.png}
    \sphrase{$h_\theta(x)) = g(\theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_1^2 + \theta_4 x_2^2$}

    \vfill
    \prevwork{Andrew Ng}
  }
  \only<2> {
    \sphrase{OvA = OvR}
    \vspace{1cm}
    \sphrase{OvO}
  }
  \only<3> {
    \sphrase{One vs All = One vs Rest}
    \vspace{1cm}
    \sphrase{One vs One}
  }
\end{frame}

\begin{frame}[t]
  \frametitle{Cost function in logistic regression}
  \vspace{1cm}
  
  \only<1-3>{
    In linear regression, we had
    
    \begin{displaymath}
      \only<1>{J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2}
      \only<2>{J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x) - y)^2}
      \only<3>{J(\theta) = \frac{1}{2m} \sum_{i=1}^m \mbox{Cost}(h_\theta(x), y)}
    \end{displaymath}
  }
  \only<4-7>{
    Here's a convex cost function:

    \begin{displaymath}
      \mbox{Cost}(h_\theta(x), y) = \begin{cases}
        -\log(h_\theta(x)) & \mbox{if } y = 1 \\[2mm]
        -\log(1-h_\theta(x)) & \mbox{if } y = 0
      \end{cases}
    \end{displaymath}
    
    \only<5-7>{\vspace{1cm}}
    \only<5>{
      \blue{Exercise: Plot this (cost vs $y$).}
    }
    \only<6>{
      \begin{displaymath}
        J(\theta) = \frac{1}{2m} \sum_{i=1}^m \mbox{Cost}(h_\theta(x), y)
      \end{displaymath}
    }
    \only<7>{
      \begin{displaymath}
        J(\theta) = y\cdot \log(h_\theta(x)) + (1-y) \cdot \log(1-h_\theta(x))
      \end{displaymath}
    }
  }
\end{frame}

\begin{frame}
  \frametitle{Gradient descent}
  \begin{bphrase}
    \begin{align*}
      \theta_j & \leftarrow\, \theta_j - \frac{\alpha}{m} \sum_{i=1}^m
                 \left(h_{\theta}(x^{(i)}) - y^{(i)}\right) \cdot x_j^{(i)} \\
    \end{align*}
    \centerline{for $j=1, \cdots, n$}
  \end{bphrase}
\end{frame}

\begin{frame}
  \only<1>{\phrase{null hypothesis}}
  \only<2>{\phrase{true positive, true negative}
    \vspace{1cm}
    \phrase{false positive, false negative}}
  \only<3>{\phrase{type I error}
  \centerline{(incorrect rejection of null hypothesis)}
    \vspace{1cm}
    \phrase{type II error}
    \centerline{(failure to reject null hypothesis)}}
  \only<4>{\phrase{sensitivity}\centerline{100\% sensitivity = no false negatives}}
  \only<5>{\phrase{specificity}\centerline{100\% specificity = no false positives}}
\end{frame}

\begin{frame}
  \frametitle{Precision}
  \begin{mphrase}
    P = \frac{TP}{TP + FP}
  \end{mphrase}
\end{frame}

\begin{frame}
  \frametitle{Recall}
  \begin{mphrase}
    R = \frac{TP}{TP + FN}
  \end{mphrase}
\end{frame}

\begin{frame}
  \frametitle{F1 score}
  \begin{mphrase}
    F1 = \frac{\text{precision}\cdot\text{recall}}{\text{precision} + \text{recall}}
  \end{mphrase}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\talksection{Break}

\begin{frame}
  \frametitle{Questions?}
  \centerline{\large\url{purple.com/talk-feedback}}
\end{frame}

\end{document}

\input ../notes-header.tex

\begin{document}

\notetitle{03}{Feature Extraction}

Categorical variables
\begin{itemize}
\item \textit{One of $K$ } or \textit{one-hot} encoding --- one binary feature per possible value
\item The importance of not encoding order where none exists
\item Text as explanatory variable $\implies$ encode as feature vectors
\item \fbox{Bag of words}
  \begin{itemize}
  \item Corpus (collection of documents)
  \item Vocabulary (set of unique words in document)
  \item Words = dimensions
  \item Order of words doesn't matter
  \item Order in vectors encodes words
  \item Binary: present or not
  \item \fbox{\tt CountVectorizer}
    \begin{itemize}
    \item by default converts to lowercase
    \item tokens
    \item stop words (mot vide) --- words in most documents don't convey much information
    \item stemming --- rule-based, drop suffixes\\
      (racinisation ou désuffixation : transformer des flexions en leur radical or racine)
    \item lemmatization -- find root form of word\\
      (lemmatisation : transformer en lemme (forme canonique))
    \end{itemize}
  \end{itemize}
\item \fbox{TF - IDF}
  \begin{itemize}
  \item \texttt{norm}: L1, L2, none (in equation: max)
  \item \texttt{use\_idf}: enable IDF, default=True
  \item \texttt{smooth\_idf}: use $n_t+1$ in IDF, default=True
  \item \texttt{sublinear\_tf}: Apply sublinear scaling, replacing $TF_{td}$ with $1+\log(TF_{td})$.
  \end{itemize}
\item \fbox{Exercise}: \texttt{HashingVectorizer}
  \begin{itemize}
  \item Problems:
    \begin{enumerate}
    \item Two passes to create structure: learn vocabulary (tokens), then create feature vectors
    \item Vocabulary (dict) stored in memory
    \end{enumerate}
  \item Instead, \texttt{HashingVectorizer}
    \begin{itemize}
    \item Bounded memory (no dict), even low memory (sparse scipy matrix)
    \item Stateless, so can be used online (streaming) and parallel
    \item Fast to serialize/unserialize
    \item n\_features defaults to $2^{20}$
    \item Note negative values.  Increment takes sign of hash value, so possibility of cancellation.
    \item \textbf{But} can't compute inverse transform, so hard to know which features are most important
    \item Collisions can happen, but rare for $2^{20}$
    \item No IDF weighting, since IDF is stateful
    \end{itemize}
  \end{itemize}

OCR
\begin{itemize}
\item sklearn.digits
  \begin{itemize}
  \item over 1700 hand-written digits (0--9)
  \item $8\times 8$ four bit pixels
  \item white is most intense and represented by 0
  \item black is least intense and represented by 16
  \end{itemize}
\item Feature vectors
  \begin{itemize}
  \item In general, matrices not sparse
  \item $100\times 100 \implies 1e4$
  \item $1920\times 1080 \implies 2e6$
  \item Problems: space, time
  \item More problems: sensitive to position, rotation, scale
  \item Even more problems: sensitive to illumination
  \item We'll come back to this with SVM (machine à vecteurs de support)\dots
  \end{itemize}
\item Corners and edges
  \begin{itemize}
  \item Basic computer vision techniques
  \item Define: feature extraction
  \item Define: feature engineering
  \item Compression
  \item Point matching
  \item Edge detectors are mostly rotation invariant
  \item So therefore corner detectors are, too
  \item But scaling can hide corners
  \end{itemize}
\item SIFT
  \begin{itemize}
  \item Uses scale-space
  \item Approximates Laplacian of Guassian with Difference of Gaussian for finding scale space
  \item Maybe a bit slower than we'd like
  \end{itemize}
\item SURF
  \begin{itemize}
  \item Speeds up SIFT
  \item Approximates Laplacian of Gaussian wth Box Filter for finding scale space
  \item \textbf{Example}
  \item Standardized dataset: zero mean, unit variance (why?)
  \item $(x-\mu)/\sigma$
  \end{itemize}
\end{itemize}
\end{itemize}

\end{document}

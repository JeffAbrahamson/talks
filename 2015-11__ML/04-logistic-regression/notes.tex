\input ../notes-header.tex

\begin{document}

\notetitle{04}{Logistic Regression}

\fbox{linear vs logistic ($\times 2$)}
\begin{itemize}
\item $y$ is discrete: classification
\item Examples:
  \begin{itemize}
  \item spam/non-spam
  \item transaction: fraud or legitimate
  \item tumor: malignant or benign
  \end{itemize}
\item So 0 or 1
\item Problems with linear regression here (picture)
\item \fbox{sigmoid ($\times 2$)}
\end{itemize}

\fbox{Non-linear decision boundaries ($\times 2$)}
\begin{itemize}
\item Still just use gradient descent \textit{(algorithme du gradient) (de la plus forte pente) (de la plus profonde descente) (hill climbing)}
\item This is why we like things to be differentiable
\item Multinomial (multi-class) classification
  \begin{itemize}
  \item one vs all (OvO, OvR) (draw picture, get three classifiers)
    \begin{itemize}
    \item At decision time, try $k-1$ classifiers, choose the one with
      the most $+1$ votes (highest probability)
    \item Problem: learners see more negatives than positives
    \item Problem: different confidence for difference decision boundaries
    \end{itemize}
  \item one vs one (OvO) (draw picture, get three classifiers)
    \begin{itemize}
    \item At decision time, try $k(k-1)/2$ classifiers, choose the one with
      the most $+1$ votes (highest probability)
    \end{itemize}

  \end{itemize}

\end{itemize}

\fbox{Cost function ($\times 7$)}
\begin{itemize}
\item This is not convex
\item So potentially many local minima
\item \fbox{Plot cost} and explain what it means for $y\in\{0,1\}$.
\item Note that our convex cost function
  \begin{itemize}
  \item is differentiable
  \item can be derived from statistics using the principles of maximum
    likelihood estimation (maximum de vraisemblance)
  \end{itemize}
\end{itemize}

\textbf{Exercises}

Error types
\begin{itemize}
\item Null hypothesis \textit{(hypothèse nulle)}
\item True/false positive/negative
\item Type I error = incorrect rejection of null hypothesis (roughly, false positives)
\item Type II error = failure to reject null hypothesis (roughly, false negatives)
\item 100\% sensitivity = no false negatives
\item 100\% specificity = no false positives
\end{itemize}


Performance metrics
\begin{itemize}
\item \fbox{Precision} (also: sensitivity)
\item \fbox{Recall}
\item Accuracy
\item F1 measure
\item \textit{skip this:}
  \begin{itemize}
  \item ROC (receiver operating characteristics = fonction
    d’efficacité du récepteur, courbe ROC) AUC (area under curve =
    l'aire sous la courbe)
  \item Inventé pendant la WWII pour montrer la séparation entre les
    signaux radar et le bruit de fond.
  \end{itemize}
\item Confusion matrix \textit{(matrice de confusion)} or contingency
  table, error matrix. Rows are classes, columns are predicted
  classes.
\end{itemize}

ROC
\begin{itemize}
\item x-axis is FPR, y-axis is TPR
\item Plots cdf of TPR against cdf of FPR
\item Really, plotting both against some classifier parameter
\item Random guessing gives point on diagonal line.  Coin flipping evolves towards $(.5, .5)$.
\item Want: just an upper left point
\end{itemize}

\end{document}

\input ../notes-header.tex

\begin{document}

\notetitle{02}{Linear Regression}

\fbox{The simple explanation ($\times 8$)} \hspace{2mm} \textit{credit: reddit}
\begin{enumerate}
\item We have 2 colors of balls on the table that we want to separate.
\item We get a stick and put it on the table, this works pretty well right?
\item Bad guy puts more balls on the table, it kind of works but one
  of the balls is on the wrong side and there is probably a better
  place to put the stick now.
\item SVMs try to put the stick in the best possible place by having
  as big a gap on either side of the stick as possible.
\item Now when the villain returns the stick is still in a pretty
  good spot.
\item There is another trick in the SVM toolbox that is even more
  important. Say the villain has seen how good you are with a stick
  so he gives you a new challenge.
\item Thereâ€™s no stick in the world that will let you split those
  balls well, so what do you do? You flip the table of course!
  Throwing the balls into the air. Then, with your pro ninja skills,
  you grab a sheet of paper and slip it between the balls.
\item Now, looking at the balls from where the villain is standing,
  they balls will look split by some curvy line.
\end{enumerate}

Vocabulary
\begin{itemize}
\item Balls = data
\item Stick = classifier (also: hyperplane)
\item Biggest gap = optimization
\item Table jump = kerneling
\item Paper = hyperplane\\[2mm]
\item Balls near margin = support vectors
\end{itemize}

\textbf{video: svm-with-polynomial-kernel.mp4}

What is wonderful and beautiful
\begin{itemize}
\item Only depends on support vectors
\item Only depends on dot products of vectors, so kernel functions are
  possible ($K(x_i\cdot x_j) = \phi(x_i)\cdot \phi(x_j)$)
\end{itemize}


\textbf{History}
\begin{itemize}
\item Invented by Vladimir Vapnik
\item PhD in USSR early 1960's
\item Thought kernels weren't very important
\item No computers to test his theories
\item Worked at an Oncology Institute developing applications
\item Invited to Bell Labs, decided to move to the U.S. in 1991
\item 1992: three papers to NIPS (Neural Information Processing Systems), refused
\item Bell Labs interested in handwriting recognition
\item 1993: Dinner bet that SVM's can do handwriting recognition better than ANN's
\item Colleague succeeds with kernel with $n=2$
\item \prevwork{\url{https://www.youtube.com/watch?v=_PwhiWxHK8o}}
\item Since 2014 at Facebook
\end{itemize}






\end{document}

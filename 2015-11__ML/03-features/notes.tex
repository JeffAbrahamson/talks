\input ../notes-header.tex

\begin{document}

\notetitle{03}{Feature Extraction}

Categorical variables
\begin{itemize}
\item \textit{One of $K$ } or \textit{one-hot} encoding --- one binary feature per possible value
\item The importance of not encoding order where none exists
\item Text as explanatory variable $\implies$ encode as feature vectors
\item \fbox{Bag of words}
  \begin{itemize}
  \item Corpus (collection of documents)
  \item Vocabulary (set of unique words in document)
  \item Words = dimensions
  \item Order of words doesn't matter
  \item Order in vectors encodes words
  \item Binary: present or not
  \item \fbox{\tt CountVectorizer}
    \begin{itemize}
    \item by default converts to lowercase
    \item tokens
    \item stop words (mot vide) --- words in most documents don't convey much information
    \item stemming --- rule-based, drop suffixes\\
      (racinisation ou d√©suffixation : transformer des flexions en leur radical or racine)
    \item lemmatization -- find root form of word\\
      (lemmatisation : transformer en lemme (forme canonique))
    \end{itemize}
  \end{itemize}
\item \fbox{TF - IDF}
  \begin{itemize}
  \item \texttt{norm}: L1, L2, none (in equation: max)
  \item \texttt{use\_idf}: enable IDF, default=True
  \item \texttt{smooth\_idf}: use $n_t+1$ in IDF, default=True
  \item \texttt{sublinear\_tf}: Apply sublinear scaling, replacing $TF_{td}$ with $1+\log(TF_{td})$.
  \end{itemize}
\item \fbox{Exercise}: \texttt{HashingVectorizer}
  \begin{itemize}
  \item Problems:
    \begin{enumerate}
    \item Two passes to create structure: learn vocabulary (tokens), then create feature vectors
    \item Vocabulary (dict) stored in memory
    \end{enumerate}
  \item Instead, \texttt{HashingVectorizer}
    \begin{itemize}
    \item Bounded memory (no dict), even low memory (sparse scipy matrix)
    \item Stateless, so can be used online (streaming) and parallel
    \item Fast to serialize/unserialize
    \item n\_features defaults to $2^{20}$
    \item Note negative values.  Increment takes sign of hash value, so possibility of cancellation.
    \item \textbf{But} can't compute inverse transform, so hard to know which features are most important
    \item Collisions can happen, but rare for $2^{20}$
    \item No IDF weighting, since IDF is stateful
    \end{itemize}
  \end{itemize}



\item 
\end{itemize}

\end{document}
